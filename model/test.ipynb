{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from solvers import power_method, uball_project\n",
    "from utils   import pre_process_3d, post_process_3d\n",
    "\n",
    "def ST(x,t):\n",
    "    \"\"\" shrinkage-thresholding operation. \n",
    "    \"\"\"\n",
    "    return x.sign()*F.relu(x.abs()-t)\n",
    "\n",
    "class CDLNetVideo(nn.Module):\n",
    "    \"\"\" Convolutional Dictionary Learning Network for Video Denoising:\n",
    "    Interpretable denoising DNN with adaptive thresholds for robustness.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 K=3,            # num. unrollings\n",
    "                 M=64,           # num. filters in each filter bank operation\n",
    "                 P=7,            # cubic filter side length\n",
    "                 s=1,            # stride of convolutions\n",
    "                 C=1,            # num. input channels\n",
    "                 t0=0,           # initial threshold\n",
    "                 adaptive=False, # noise-adaptive thresholds\n",
    "                 init=True):     # False -> use power-method for weight init\n",
    "        super(CDLNetVideo, self).__init__()\n",
    "        \n",
    "        # -- OPERATOR INIT --\n",
    "        self.A = nn.ModuleList([nn.Conv3d(C, M, P, stride=s, padding=(P-1)//2, bias=False) for _ in range(K)])\n",
    "        self.B = nn.ModuleList([nn.ConvTranspose3d(M, C, P, stride=s, padding=(P-1)//2, output_padding=s-1, bias=False) for _ in range(K)])\n",
    "        self.D = self.B[0]                              # alias D to B[0], otherwise unused as z0 is zero\n",
    "        self.t = nn.Parameter(t0 * torch.ones(K, 2, M, 1, 1, 1)) # learned thresholds (added one more dimension)\n",
    "        \n",
    "        # set weights \n",
    "        W = torch.randn(M, C, P, P, P)\n",
    "        for k in range(K):\n",
    "            self.A[k].weight.data = W.clone()\n",
    "            self.B[k].weight.data = W.clone()\n",
    "        \n",
    "        # Don't bother running code if initializing trained model from state-dict\n",
    "        if init:\n",
    "            print(\"Running power-method on initial dictionary...\")\n",
    "            with torch.no_grad():\n",
    "                DDt = lambda x: self.D(self.A[0](x))\n",
    "                L = power_method(DDt, torch.rand(1, C, 16, 128, 128), num_iter=200, verbose=False)[0]\n",
    "                print(f\"Done. L={L:.3e}.\")\n",
    "                \n",
    "                if L < 0:\n",
    "                    print(\"STOP: something is very very wrong...\")\n",
    "                    sys.exit()\n",
    "                \n",
    "            # spectral normalization (note: D is alised to B[0])\n",
    "            for k in range(K):\n",
    "                self.A[k].weight.data /= np.sqrt(L)\n",
    "                self.B[k].weight.data /= np.sqrt(L)\n",
    "        \n",
    "        # set parameters\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "        self.P = P\n",
    "        self.s = s\n",
    "        self.t0 = t0\n",
    "        self.adaptive = adaptive\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def project(self):\n",
    "        \"\"\" \\ell_2 ball projection for filters, R_+ projection for thresholds\n",
    "        \"\"\"\n",
    "        self.t.clamp_(0.0)\n",
    "        for k in range(self.K):\n",
    "            self.A[k].weight.data = uball_project(self.A[k].weight.data, dim=(2,3,4)) #onto the unit ball for 3D convolutions\n",
    "            self.B[k].weight.data = uball_project(self.B[k].weight.data, dim=(2,3,4))\n",
    "\n",
    "    def forward(self, y, sigma=None, mask=1):\n",
    "        \"\"\" LISTA + D w/ noise-adaptive thresholds\n",
    "        \"\"\"\n",
    "        yp, params, mask = pre_process_3d(y, self.s, mask=mask)\n",
    "        \n",
    "        # THRESHOLD SCALE-FACTOR c\n",
    "        c = 0 if sigma is None or not self.adaptive else sigma / 255.0\n",
    "        \n",
    "        # LISTA\n",
    "        z = ST(self.A[0](yp), self.t[0, :1] + c * self.t[0, 1:2])\n",
    "        for k in range(1, self.K):\n",
    "            z = ST(z - self.A[k](mask * self.B[k](z) - yp), self.t[k, :1] + c * self.t[k, 1:2])\n",
    "        \n",
    "        # DICTIONARY SYNTHESIS\n",
    "        xphat = self.D(z)\n",
    "        xhat = post_process_3d(xphat, params)\n",
    "        return xhat, z\n",
    "\n",
    "    def forward_generator(self, y, sigma=None, mask=1):\n",
    "        \"\"\" same as forward but yields intermediate sparse codes\n",
    "        \"\"\"\n",
    "        yp, params, mask = pre_process_3d(y, self.s, mask=mask)\n",
    "        c = 0 if sigma is None or not self.adaptiave else sigma / 255.0\n",
    "        z = ST(self.A[0](yp), self.t[0, :1] + c * self.t[0, 1:2]); yield z\n",
    "        for k in range(1, self.K):\n",
    "            z = ST(z - self.A[k](mask * self.B[k](z) - yp), self.t[k, :1] + c * self.t[k, 1:2]); yield z\n",
    "        xphat = self.D(z)\n",
    "        xhat = post_process3d(xphat, params)\n",
    "        yield xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running power-method on initial dictionary...\n",
      "Done. L=3.161e+04.\n"
     ]
    }
   ],
   "source": [
    "model = CDLNetVideo(\n",
    "    K=3,\n",
    "    M=64,\n",
    "    P=7,\n",
    "    s=1,\n",
    "    C=1,           \n",
    "    t0=0.1,\n",
    "    adaptive=True,\n",
    "    init=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 1\n",
    "depth = 16\n",
    "height = 128\n",
    "width = 128\n",
    "dummy_input = torch.randn(batch_size, channels, depth, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, z = model(dummy_input, sigma=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t - Grad shape: torch.Size([3, 2, 64, 1, 1, 1])\n",
      "A.0.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n",
      "A.1.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n",
      "A.2.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n",
      "B.0.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n",
      "B.1.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n",
      "B.2.weight - Grad shape: torch.Size([64, 1, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn_like(output)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f'{name} - Grad shape: {param.grad.shape}')\n",
    "    else:\n",
    "        print(f'{name} - No gradient')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
